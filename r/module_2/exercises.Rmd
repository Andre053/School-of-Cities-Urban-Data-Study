---
title: "module_2"
output: html_document
---

# Data processing and analytics
Part of module 2 uses three datasets to help teach common data operations. These datasets are on New York cities, cities, and capital cities.

First, import useful libraries:
```{r}
library(tidyr)
library(janitor)
library(ggplot2)
library(dplyr)
```

Then import data from the files:
```{r}
df_cities <- read.csv("./data/cities.csv") |> clean_names()
df_capitals <- read.csv("./data/capitals.csv") |> clean_names()
df_ny_cities <- read.csv("./data/new_york_cities.csv") |> clean_names()
```

Prepare the data:
```{r}
# column should be bool
df_capitals <- df_capitals |>
  mutate(is_capital=ifelse(is_capital=="True", TRUE, FALSE))
```


## Filtering exercises
Selecting only the rows that meet conditions of interest. Good filtering must be clear on what data should be kept. 

Key questions to ask are:
1. What do you want to understand?
2. Which records help answer the questions you have?

**Q1: How many cities are there with a population of 100,000 or more in 2021?**
I want to understand the number of cities with a population of 100,000 or more in 2021. The cities df has a feature perfect for this question, giving the population of the city in 2021. I can filter this feature by this condition and count how many rows remain. 
```{r}
df_cities |>
  select(population_2021) |>
  filter(population_2021 >= 100000) |>
  count()
```
Of the 179 cities, 56 have populations above 100,000 in 2021. 

**Q2: How many cities are in Alberta?**
To understand the number of cities in Alberta, I need to filter all records where the prov_terr is equal to Alberta and count the records that remain. First, the abbreviation for Alberta used in the column must be found, then filter by it and count the records. 
```{r}
# find abbreviation for Alberta
df_cities$prov_terr |> unique()

# filter by the abbreviation "Alta."
df_cities |>
  select(prov_terr) |>
  filter(prov_terr=="Alta.") |>
  count()
```
17 out of 179 cities are in Alberta.

## Sorting exercises
Arranging rows can help reveal patterns and highlight key observations. How you sort should depend on which order is best to help answer your analytical question.

**Q1: Which 3 cities had the most population in 2021?**
To get the top 3 populous cities in 2021, the name and population_2021 features are necessary. With these columns, I can sort by the population and then take the top 3 cities.
```{r}
df_cities |>
  select(name, population_2021) |>
  arrange(desc(population_2021)) |>
  slice_head(n=3)
```
The top 3 cities by population in 2021 are Toronto, MontrÃ©al, and Calgary. 

**Q2: What provinces/territories do the top 10 cities ranked by population in 2021 belong to?**
The features needed are prov_terr and 2021 population. After ranking by population and taking the top 10, the answer is the unique list of prov_terr values. 
```{r}
df_cities |>
  select(prov_terr, population_2021) |>
  arrange(desc(population_2021)) |>
  slice_head(n=10) |>
  select(prov_terr) |>
  unique()
```
These cities belong to Ontario, Quebec, Alberta, Manitoba, and B.C.

## Joining datasets exercises
By joining datasets, the information available in a single table is increased, potentially enriching analysis.

Two common types of joins:
- Vertical concatenation: Stacking new rows onto a table where the columns are the same
- Horizontal merging: Combining datasets with a shared key, adding new columns for existing records

Good joining practice:
- Ensure keys align correctly
- Handle missing matches
- Maintain data integrity

**Q1: Use vertical concatenation to add NY cities into the cities dataframe. How many rows are there now?**
This could be found by simply adding the rows of each dataframe. The intended solution is to add the NY cities data to the cities data and count the number of observations. 
```{r}
df_cities |>
  rbind(df_ny_cities) |> # used to vertically concat dfs in R
  count()
```
There are 181 rows after vertically concatenating the tables. 

**Q2: What are the top 3 cities by 2021 population?**
With the concatenated data, select name and 2021 population, sort by population, and take the top 3 observations. 
```{r}
df_cities |>
  rbind(df_ny_cities) |> 
  select(name, population_2021) |>
  arrange(desc(population_2021)) |>
  slice_head(n=3)
```
After concatenating the data tables, NYC has taken the top spot, with Calgary falling out. 

**Q3: Horizontally merge cities with capitals on the column 'Name'. How many cities are capitals in this dataset?**
Merge the cities table with capitals; it is an inner join that only keeps records that appear in both tables. Now count those that are capital cities. If this was meant to be a left join, the same info could be found by filtering where is_capital is equal to TRUE and counting the observations. 
```{r}
df_cities |>
  merge(df_capitals, by="name") |>
  filter(is_capital==TRUE) |>
  count()
```
There are 11 capital cities. 

## Handling missing data exercises
When there is missing or incomplete data you must use your best judgement on how to handle it. There are some common approaches for how to move forward:
- Remove incomplete data: Simple and effective if there are few missing values. Can lead to misleading analyses
- Imputation of replacement: Fill incomplete records with constants, averages, or modeled estimates. Introduces assumptions to the data
- Selective handling: Only handle missing data in essential columns

**Q1: How many rows have missing data in 'Name'?**
Select the name column and count rows where the value is an empty string. These could have been filled with NA when the data was loaded in.  
```{r}
df_cities |>
  select(name) |>
  filter(name=="") |>
  count()
```
There are 3 records with missing name data. 

**Q2: In 'Prov/terr'?**
Do the same for province/territory
```{r}
df_cities |>
  select(prov_terr) |>
  filter(prov_terr=="") |>
  count()
```
There are 4 records with missing province/territory data. 

**Q3: Remove every row which has missing data. How many remain?**
Filter out all rows where there is an NA value. If all columns used NA to signify missing data, the code could be simplified. 
```{r}
df_cities |>
  filter(
    name != "",
    prov_terr != "",
    ! is.na(population_2016),
    ! is.na(population_2021)
  ) |>
  count()
```
There are 167 records without missing data.

## Summary statistics exercises
Descriptive statistics outline the main characteristics of the data, such as distribution, central tendencies, and variation.

Common summary measures:
- Count: Number of observations available
- Mean and median: Central tendency values
- Min, max, percentiles: Extremes and spread
- Standard deviation and variance: Spread of different data points

**Q1: What is the mean population for 2016?**
Get 2016 population data and calculate the mean, ignoring NA values. 
```{r}
mean(df_cities$population_2016, na.rm = TRUE)
```
Cities have, on average, 144,168.6 people in 2016.

**Q2: What is the minimum population for 2016?**
Find the minimum of the 2016 population data. 
```{r}
min(df_cities$population_2016, na.rm = TRUE)
```
In 2016, the smallest city in the dataset had a population of 23,787 people.

## Grouping and aggregation exercises
Move from individual records to purposeful groupings and aggregate statistics.

Grouped workflow:
1. Choose a variable to group by
2. Select one or more descriptive measures to compute

*No exercises*

## Calculating new variables exercises
Oftentimes, you may want to create new variables based off of the raw data.

Useful derived measures:
- Standardized comparisons
- Adjust for scale
- Express quantities in better ways

Common derived measures:
- Percentages and rates
- Ratios

**Q1: Calculate each city's population change between 2016 and 2021 by creating a new column called 'PctPopChange'. What is the percentage change in Toronto?**
Create a new column that is the percentage change between 2016 and 2021 populations. This is done dividing the 2021 population by the 2016 population, multiplying by 100 (for % value), then subtracting 100 to get the % change. Additionally, round each value to two decimal places as requested by the question. Finally, filter the data and isolate the value for Toronto. 
```{r}
df_cities |>
  mutate(
    pct_pop_change=round(
      ((population_2021) / (population_2016) * 100) - 100, 
      digits = 2
  ))  |>
  select(name, pct_pop_change) |>
  filter(name=="Toronto")
  
```
























